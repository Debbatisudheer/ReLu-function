<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ReLU Activation Function</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="background-image">
    <div class="container">
      <h1>Rectified Linear Unit (ReLU)</h1>
      <img src="reluu.png" alt="Another Image">
      <p><strong>Purpose:</strong> Outputs the input directly if positive, zero otherwise, facilitating faster learning.</p>
      <p><strong>When to Use:</strong> Widely used in hidden layers of deep neural networks for its simplicity and effectiveness.</p>
      <p>Imagine you have a box, and you want to decide whether to keep what's inside or throw it away. If what's inside is already good (positive), you just keep it as it is. But if it's not good (negative), you just throw it away and keep nothing.</p>
      <p>That's what Rectified Linear Unit (ReLU) does in a neural network. It's like this box: if the input it receives is positive, it passes it through without changing anything. But if the input is negative, it simply sets it to zero, meaning it throws it away.</p>
    </div>
  </div>
</body>
</html>


